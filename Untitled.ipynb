{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sql\n",
    "from pyspark.sql.types import DoubleType\n",
    "import os\n",
    "import sys\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "from pyspark.ml.pipeline import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('TrainingDatasetRaw.csv', 'r') \n",
    "file2 = open('training.csv', 'w')\n",
    "Lines = file1.readlines() \n",
    "  \n",
    "count = 0\n",
    "# Strips the newline character \n",
    "for line in Lines: \n",
    "    line2 = line.replace(';', ',')\n",
    "    line3 = line2.replace('\"', '')\n",
    "    n = file2.write(line3)\n",
    "file1.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
